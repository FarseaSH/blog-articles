---
date: 2017-08-12 20:45:42
title: Programming with Python for Data Science学习笔记
category: 经验
tags:
  - Pandas
  - Python
  - Scikit-Learn
  - 机器学习
toc: true
copyright: true
---

本文写于本人在[EdX](https://www.edx.org/)平台上学习[Programming with Python for Data Science](https://www.edx.org/course/programming-python-data-science-microsoft-dat210x-5)课程过程中。该课程主要讲述如何利用Python进行数据科学实践，内容涵盖Pandas、Sciki-learn等数据科学中常用包的主要命令，并结合实际数据集进行实战操作。在学习过程中，我将课程中学习到、代码作业中所遇到的命令在此进行归纳总结，以方便后面学习者查阅。另外该门课程中所用到命令也常在实际操作中用到，亦可将本文作为数据科学工作中的参考资料。
<!--more-->
本文按照课程学习顺序对命令进行排序。另外命令方法的参数并未列全，只列出常用参数，更多参数功能可参见官方文档。

另外，我将课程Lab上机作业的代码放在我Github上托管[DAT210](https://github.com/shaoyy147/DAT210x-python_datascience)，若有需要，可供参考。

## 数据预处理

### 数据读入

### 读入csv等文本文件`pd.read_csv()`
```python
import pandas as pd

df = pd.read_csv('FilePath', header = None, names = ['ColunName_1', 'ColunName_N' ], sep = ',', index_col = 0, na_values = '?')
```
读入数据文件，转换为`Dataframe`格式，赋予`df`名称。位置参数`'FilePath'`指定数据文件路径；关键字参数`header`指定文件中，数据Feature的名称所在的行数，`None`表示数据文件中没有包含Feature的名称；参数`names`手动赋予Feature的名称；参数`sep`指定数据文件中的分隔符；参数`index_col`指定样本标签（唯一确定某条样本信息）所在的列数。

#### 读入html网页中的表格`pd.read_html()`

```python
df = pd.read_html('URL')[0]
```

从网页中读入**一系列**表格，转换为`Dataframe`的`List`，取其中元素赋予`df`名称。位置参数`'URL'`指定读入网页的网址，**注意**可能需将*https*改为*http*，以匹配符合的协议。

#### 读入图像文件

```python
from scipy import misc

sample = []
img = misc.read('FilePath')    #读取文件
sample.append((img / 255.0).reshape(-1))      #将文件转换至一维数组
df = pd.DataFrame(sample)          #转化为Dataframe格式
```



### Dataframe数据集概览操作

#### 浏览具体数据`df.head()`与`df.tail()`

```python
df.head(Num)
df.tail(Num)
```

返回`df`中前/后几行数据具体情况。位置参数`Num`指定行数。

#### 数值特征的统计量`df.describe()`

```python
df.describe()
```

返回`df`中数值特征的相关统计量，包括样本量、最大（小）值、平均值、标准差、四分位数。在统一Feature Scaling 前可以用此命令观察数据的特点。

#### 浏览数据集中的特征`df.columns`与`df.dtypes`

```python
df.colunms
df.dtypes
```

`df,columns`返回`df`中特征的名称,`df.dtypes`返回`df`中特征的名称以及对应储存数据的类型。在导入数据过程中，可能因为各种原因所期望的数据类型与实际数据类型不同，会影响后面命令的执行，这时需要对进行数据类型转换。**注意**上述命令是`df`的属性，而非`df`的方法，即命令后无`()`。

#### Feature的所有取值`df.feature.unique()`

```python
df.feature_name.unique()
```

返回数据集`df`的`feature_name`属性的所有取值。

#### 数据集切片

常规切片

```python
#选取某些纵列，即选取全部样本的某些Feature
#以下命令返回Series对象,假设选取Feature名为'recency'的第0纵列，
df.recency
df['recency']
df.loc[:,'recency']
df.iloc[:,0]
df.ix[:,0]

#以下命令返回Dataframe对象
df.[['recency']]
df.loc[:,['recency']]
df.iloc[:,[0]]

#选取某些行，即选取某些样本的全部Feature
df[0:2]
df.loc[0:2,:]
```

条件选取切片

```python
df[(df.recency < 7) & (df.newbie == 0)]
```

返回`df`数据集中`'recency'`属性小于7且`'newbie'`属性等于0的样本

### 数据集整理

#### 数据集Feature名称修改

```python
df.columns = ['Feature_1','Feature_2']
```

#### 有序分类Feature转换

```python
#下面以satisfaction属性为例
ordered_satisfaction = ['Very Unhappy', 'Unhappy', 'Neutral', 'Happy', 'Very Happy']
df.satisfaction = df.satisfaction.astype("category",
                                         ordered=True,categories=ordered_satisfaction).cat.codes
```

将`df`数据集中分类Feature`satisfaction`从文本属性转换为整数表示，1对应`'Very Unhappy'`，5对应`'Very Happy'`，-1表示无对应。`astype("category", ordered = True, categories = ordered_satisfaction)`方法将`Series`按照`categories`参数转换为有序的`category`类型，`cat.codes`为`category`类型所对应的数字编码。

#### 无序分类Feature转换`pd.get_dummies()`

```python
df = pd.get_dummies(df,columns=['column_1','column_2'])
```

将数据集`df`中无序分类Feature`column_1`和`column_2`转换为dummy表示(0/1表示)。位置参数`df`指定待操作的数据集；关键字参数`columns`指定待操作的Feature。

#### 缺省Nan值替换`df.fillna()`与`df.feature.fillna()`

```python
#df.fillna()与df.feature.fillna()参数相同，下例只说明前者
df.fillna(0)
df.fillna(method = 'ffill', limit = 5)
```

第一行命令将数据集`df`中的所检索到的Nan值用0替换掉。第二行命令将数据集`df`中所检索到的Nan值用前一条样本的对应值替代，关键字参数`method`指定替代的方法，`limit`指定替换的最大间隔（在连续多个数据都为Nan时用到），具体方法参见[官方文档](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)。

#### 数据剔除`df.drop()`与`df.dropna()`

```python
df = df.dropna(axis = 0, thresh = 5)
df = df.drop(labels=['column_1','column_2'],axis = 1)
```

第一行命令将`df`数据集中有5项及其以上属性为Nan的数据剔除，关键字参数`axis`指定剔除数据集的行（样本）或列（属性），0表示剔除行，1表示剔除列，`thresh`指定当出现多少Nan值时剔除该项。第二行命令剔除`df`数据集中`'column_1'`和`'column_2'`属性，关键字参数`label`指定所剔除的项,`axis`指定剔除数据集的行（样本）或列（属性），0表示剔除行，1表示剔除列。

#### 数据类型转化`pd.to_numeric()`

```python
df.Num = pd.to_numeric(df.Num, errors = 'coerce')
```

将数据集`df`中`Num`属性下数据转换为相应的数值类型。位置参数指定所有转化的`Series`，关键字参数`errors`指定处理错误的方式，`'coerce'`将无法转化的值转化为Nan。在读取数据时，需要特别注意，往往所期望的数据类型与实际的数据类型不一致，这会导致数据处理上的错误，这要求需要对处理数据前，检查数据类型，并转化为所期望的数据类型。

#### 数据编号重排`reset_index()`

```python
df.reset_index(drop = True, inplace = True)
```

将数据集`df`的编号重新编排。关键字参数`drop`指定是否保留原有的编号，参数`inplace`指定是否在原有数据集上操作，或返回一个新的数据集。当对样本进行剔除后，数据集样本编号不会随之变动，利用`reset_index()`方法对数据集编号进行重新编排。

#### 数据特征缩放Feature Scaling

```python
from sklearn.preprocessing import Normalizer

norm = Normalizer()
norm.fit(df)
df = norm.transform(df)
```

Normalize数据特征，使得数据在同一Scale下，在K近邻算法，PCA主成分降维算法等中常用到。除`Normalizer`方法外，常用特征放缩方法还有`MinMaxScaler`,`RobustScaler`,`StandardScaler`。

#### 数据训练集与测试集分离`train_test_split()`

```python
from sklearn.model_selection import train_test_split

data_train, data_test, label_train, label_test = train_test_split(data, labels,
                                                                  test_size=0.5)
```

将训练样本`data`与其对应标签`labels`随机分成训练集、测试集。关键字参数`test_size`指定测试集占总样本的比例。

## 数据可视化

### 基础图像绘制

#### 条形图Histograms`df.plot.hist()`

```python
import pandas as pd
import matplotlib
matplotlib.style.use('ggplot') #制定图像绘制风格

df.plot.hist(alpha = 0.5)
```

绘制数据集`df`上条形图，关键字参数`alpha`制定柱形的透明程度，0表示完全透明，1表示完全不透明。

#### 二维散点图2D-Scatter`df.plot.scatter()`

```python
import pandas as pd
import matplotlib

df.plot.scatter(x = 'feature_1', y = 'feature_2')
```

绘制数据集`df`上关于属性`'feature_1'`和`'feature_2'`的散点图，关键字参数`x`和`y`指定需要作出散点图的两个数据集的Feature。

#### 三维散点图3D-Scatter`df.plot.scatter()`

```python
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from mpl_tookits.mplot3d import Axes3D

fig = plt.figure()      #生成figure类的实例fig
ax = fig.add_subplot(111, projection = '3d')   #在fig上添加图，设置为3d坐标图
ax.set_xlabel('xlabel')    #设置xyz坐标轴的名称
ax.set_ylabel('ylabel')
ax.set_zlabel('zlabel')
ax.scatter(df.feature_1, df.feature_2, df.feature_3, c='r', marker = '.')  
#绘制三维图，关键字参数c制定散点的颜色，参数marker制定散点图案类型
plt.show()   #显示绘制图像
```

绘制关于数据集`df`的三个属性`feature_1``feature_2``feature_3`的三维散点图。**注意**`Pandas`并未自带的绘制三维散点图的方法。

### 高维数据图像绘制

#### Parallel Coordinate `parallel_coordinates()`

```python
plt.figure()
parallel_coordinates(df, 'cat_names')
plt.show()
```

绘制数据集`df`的Parallel Coordinate图像，位置参数`df`制定绘制的数据集，参数`cat_names`指定分类所依据的Feature。**注意**Matplotlib包的parallel_coordinates所绘制的所有纵坐标量级尺度都是一样，无法不同Feature不同处理。

#### Andrew's Curves `andrews_curves()`

```python
from pandas.tools.plotting import andrews_curves

plt.figure()
andrews_curves(df, 'cat_names')
plt.show()
```

绘制数据集`df`的Andrew's Curves图像，位置参数`df`制定绘制的数据集，参数`cat_names`指定分类所依据的Feature。

#### Imshow `plt.imshow()`

```python
#绘制图像
plt.imshow(df.corr(), cmap=plt.cm.Blues, interpolation='nearest')
#以下设置坐标名称
tick_marks = [i for i in range(len(df.columns))]
plt.xticks(tick_marks, df.columns, rotation='vertical')
plt.yticks(tick_marks, df.columns)
#显示图像
plt.show()
```

绘制数据集`df`相关矩阵的Imshow图像。Imshow图像可用于找出相关程度较高的一组Feature。

## 数据转换

### 主成分分析PCA

```python
from sklearn.decomposition import PCA

pca = PCA(n_components = 2, svd_solver = 'full')
pca.fit(df)
T = pca.transform(df)
```

根据设置，利用数据集`df`训练模型，并将训练后的模型应用于数据集`df`上，返回的结果储存于`T`。`PCA()`生成PCA类实例，参数`n_component`指定最后主成分个数，`svd_solver`指定模型运算算法。`fit()`方法将`df`数据集应用于模型上。`transform()`方法将`df`按照训练后的模型转化为新的数据集。

### 流形降维Isomap

```python
from sklearn import manifold

iso = manifold.Isomap(n_neighbors = 5, n_components = 3)
iso.fit(df)
T = iso.transform(df)
```

根据参数设置，利用数据集`df`训练模型，并将训练后的模型应用于数据集`df`上，返回的结果储存于`T`。`Isomap()`生成Isomap类实例，参数`n_neighbors`指定邻域节点数，参数`n_component`指定最后主成分个数。`fit()`方法将`df`数据集应用于模型上。`transform()`方法将`df`按照训练后的模型转化为新的数据集。

## 机器学习算法

### 聚类K-Means

```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters = 5, n_init = 10)
kmeans.fit(df)
kmeans.cluster_centers_
```

根据参数设置，利用数据集`df`训练模型，并将训练后的模型应用于数据集`df`上，返回的结果储存于`T`。`KMeans()`生成KMeans类实例，参数`n_clusters`指定分类数，参数`n_init`指定算法重复执行次数。`fit()`方法将`df`数据集应用于模型上。属性`cluster_centers_`返回训练后模型的各类中心点坐标。

### K近邻K-Nearest Neighbors

```python
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=3, weights = 'uniform')
model.fit(X_train, y_train) 
model.score(X_test, y_test)
```

根据参数设置，利用数据集`X_train`与其对应标签`y_train`训练模型。`KNeighborsClassifier()`生成KNeighborsClassifier类实例，参数`n_neighbors`指定预测数据距离最近的训练样本的数量，参数`weights`指定最近样本决定预测数据的方式，`uniform`表示所有样本权重一致，此外还有`distance`，表示样本按距离远近的倒数赋予相应权值。`score()`方法返回模型在测试集上的准确率。

### 线性回归Linear Regression

```python
from sklearn import linear_model

model = linear_model.LinearRegression()
model.fix(X_train, y_train)
model.score(X_test, y_test)
```

根据参数设置，利用数据集`X_train`与其对应标签`y_train`训练模型。`LinearRegression()`生成LinearRegression类实例，`score()`方法返回模型在测试集上的准确率,即计算R平方值。

### 支持向量机SVM

```python
from sklearn.svm import SVC

model = SVC(kernel = 'linear',C = 1.0 gamma = 0.1)
model.fix(X_train, y_train)
model.score(X_test, y_test)
```

根据参数设置，利用数据集`X_train`与其对应标签`y_train`训练模型。`SVC()`生成SVC类实例，参数`kernel`指定kernel函数，参数`C`指定错误惩罚程度，该参数越大，对错误样本惩罚值越高，越可能导致过度拟合。`score()`方法返回模型在测试集上的准确率,即计算R平方值。

### 决策树Decision Tree

```python
from sklearn import tree

model = tree.DecisionTreeClassifier(max_depth = 5, criterion = 'entropy')
model.fit(X_train, y_train)
tree.export_graphviz(model.tree_, out_file='tree.dot', feature_names=X.columns)
```

根据参数设置，利用数据集`X_train`与其对应标签`y_train`训练模型。`DecisionTreeClassifier()`生成DecisionTreeClassifier类实例，参数`max_depth`指定树的深度；参数`criterion`指定树分支的计算指标，默认为Gini系数，'entropy'计算商。`tree.export_graphviz()`方法输出训练后模型的可视化图像文件，该图像文件需系统中安装树可视化软件[graphviz](http://www.graphviz.org/)。

### 随机森林Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators = 10)
model.fit(X_train, y_train)
```

根据参数设置，利用数据集`X_train`与其对应标签`y_train`训练模型。参数`n_estimators`指定随机森林中数的数量。

## 模型评价与整合

### Confusion Metrix

```python
import sklearn.metrics as metrics

metrics.confusion_matrix(y_true, y_pred)
```

输出度量Confusion Metrix，每行表示每类实际结果的预测分布情况，每列表示每类预测结果的实际值分布情况。以下给出Confusion Metrix与imshow图像的结合应用的例子:

```python
import matplotlib.pyplot as plt

columns = ['Cat', 'Dog', 'Monkey']
confusion = metrics.confusion_matrix(y_true, y_pred)

plt.imshow(confusion, cmap=plt.cm.Blues, interpolation='nearest')
plt.xticks([0,1,2], columns, rotation='vertical')
plt.yticks([0,1,2], columns)
plt.colorbar()

plt.show()
```

### 验证Validation

#### 度量矩阵

```python
import sklearn.metrics as metrics
y_true = [1, 1, 2, 2, 3, 3]
y_pred = [1, 1, 1, 3, 2, 3]

# recall = true_positives / (true_positives + false_negatives)
metrics.recall_score(y_true, y_pred, average='weighted')   #average参数用于指定各类平均方法
# precision = true_positives / (true_positives + false_positives)
metrics.precision_score(y_true, y_pred, average=None)
# F1 = 2 * (precision * recall) / (precision + recall)
metrics.f1_score(y_true, y_pred, average='weighted')

target_names = ['name 1', 'name 2', 'name 3']
metrics.classification_report(y_true, y_pred, target_names=target_names)
```

#### Cross Validation

```python
from sklearn import cross_validation as cval
cval.cross_val_score(model, X_train, y_train, cv=10) #返回每组的训练集下的准确度 cv制定分组数
cval.cross_val_score(model, X_train, y_train, cv=10).mean() #返回平均值
```

### 模型参数调整`grid_search`

```python
from sklearn import grid_search

parameters = {'kernel':('linear', 'rbf'), 'C':[1, 5, 10]}
model = svm.SVC()
classifier = grid_search.GridSearchCV(model, parameters)  
classifier.fit(X_train, y_train)
#以上根据参数的可能性，进行组合比较训练，得到最佳的模型参数

parameter_dist = {
  'C': scipy.stats.expon(scale=100),
  'kernel': ['linear'],
  'gamma': scipy.stats.expon(scale=.1),
}
classifier = grid_search.RandomizedSearchCV(model, parameters_dist)  
classifier.fit(X_train, y_train)
#以上根据参数的所指定的分布，进行组合比较训练，得到最佳的模型参数
```

### 模型Pipeline

```python
from sklearn.pipeline import Pipeline
svc = svm.SVC(kernel='linear')
pca = RandomizedPCA()
pipeline = Pipeline([('pca', pca),('svc', svc)])
pipeline.set_params(pca__n_components=5, svc__C=1, svc__gamma=0.0001)
pipeline.fit(X, y)
```

将多个模型由前至后连接，整合成一个机器学习模型。需要注意的是中间模型需要有`.fit()` `.transform()`方法 ，如果原类无对应方法，可以自定义相对应的方法，以适应Pipeline的运行。